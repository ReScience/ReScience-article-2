\documentclass[a4paper,10pt, twocolumn]{article}

% Wider margins (compared to default)
\usepackage[margin=2.0cm]{geometry}

% For misc. computations
\usepackage{calc}
\usepackage{enumitem}

% English support (typography and hyphenation)
\usepackage[american]{babel}
\usepackage{csquotes}

% For math
\usepackage{amsmath}

% Unicode encoding
\usepackage[utf8]{inputenc}

% Color box
\usepackage{tikz,lipsum,lmodern}
\usepackage[most]{tcolorbox}

% Better default font (Libertine and Inconsolata)
\usepackage[ttscale=.875]{libertine}
\usepackage[scaled=0.96]{zi4}

%% \usepackage{mathspec}
%% \setmainfont{Minion Pro Cond}
%% \setmathsfont(Digits,Greek,Latin)[Numbers={Proportional}]{Minion Pro}
%% \setmathrm{Minion Pro}
%% \setsansfont{MyriadPro-Cond}

\usepackage{nicefrac}

% Biber
\usepackage[backend=biber,
            style=apa,
%            maxcitenames=3,
%            uniquelist=false,
%            maxbibnames=99,
%            apabackref=false,
            apamaxprtauth=99,
            natbib=true]{biblatex}
\DeclareLanguageMapping{american}{american-apa}



% Graphics
\usepackage{graphicx}

% Figure caption
\usepackage[labelsep=period]{caption}
\renewcommand{\captionfont}{\small\sffamily}
\renewcommand{\captionlabelfont}{\small\sffamily\bfseries}

% Hyperref
\usepackage{xcolor}
\definecolor{blendedblue}{rgb}{0.2, 0.2, 0.6}
\definecolor{blendedred}{rgb}{0.8, 0.2, 0.2}
\usepackage[bookmarks=true,
            breaklinks=true,
            pdfborder={0 0 0},
            citecolor=blendedblue,
            colorlinks=true,
            linkcolor=blendedblue,
            urlcolor=blendedblue,
            citecolor=blendedblue,
            linktocpage=false,
            hyperindex=true,
            linkbordercolor=white]{hyperref}
\usepackage{hyperref}
\hypersetup{colorlinks=true}


% Bibliography file
\bibliography{article.bib}

% Headers
\usepackage{fancyhdr}
\pagestyle{fancy}
% \fancyhf{}
\rhead{\footnotesize \sf page \thepage}
\lhead{\footnotesize \sf Rougier, Hinsen et al. 2017
       \textbullet~The ReScience Initiative}
\rfoot{}
\cfoot{}
\lfoot{}

\makeatletter
\renewcommand{\maketitle}{\bgroup\setlength{\parindent}{0pt}
\begin{flushleft}
  \textbf{\huge\@title\\}
  \vspace{5mm}
  \@author
\end{flushleft}\egroup
}
\makeatother


%% \newtcbox{\orcid}{enhanced,nobeforeafter,tcbox raise base,boxrule=0.4pt,top=0mm,bottom=0mm,
%%   right=0mm,left=4mm,arc=1pt,boxsep=2pt,before upper={\vphantom{dlg}},
%%   colframe=green!50!black,coltext=green!25!black,colback=green!10!white,
%%   overlay={\begin{tcbclipinterior}\fill[green!75!blue!50!white] (frame.south west)
%%     rectangle node[text=white,font=\sffamily\bfseries\tiny,rotate=0] {ID} ([xshift=4mm]frame.north west);\end{tcbclipinterior}}}

\newcommand{\hgap}{\hspace{1mm}}

\usepackage{tikz}
\usetikzlibrary{positioning,shapes,shadows,arrows}
\newcommand{\orcid}[1]{
    \tikz[baseline=-0.5ex]{ 
        \tikzset{lib/.style={
            rectangle split,
            rectangle split parts=2,
            rectangle split horizontal,
            rectangle split part fill={green!75!blue!50!white,green!10!white},
            rectangle split draw splits=false,
            rounded corners=1pt,
            rectangle split part align={right,right},
            draw=green!50!black,
            minimum height=8pt}
        }
        \node[lib] (var){
        \nodepart[text=green!25!black]{two} \footnotesize \href{http://orcid.org/#1}{#1}};
        \node[text=white,font=\sffamily\bfseries\footnotesize,rotate=0] at ([xshift=6pt]var.west) {ID};
    }
}

\title{Long-Term Reproducibility through Replication:\\
       the ReScience Initiative}
\author{%
  \begin{small}
    \input{authors.tex}
  \end{small}
\begin{footnotesize}
  \vspace{2mm}
  \input{affiliations.tex}
  $^{*}$Corresponding author:
        \href{mailto:Nicolas.Rougier@inria.fr}{Nicolas.Rougier@inria.fr}          
  \end{footnotesize}
}

\date{}

\begin{document}

\twocolumn[
\maketitle
\begin{small}
  \noindent
  \textbf{} \par
  \textbf{Keywords:} Open Science, Computational Science,
                     Reproducibility, Replicability
\end{small}
\vspace{10mm}
]


\section*{Introduction}
There is a replication crisis in Science. It has recently been highlighted in
medicine \citep{ioannidis:2005}, psychology \citep{nosek:2015}, the
political sciences \citep{janz:2015}, and even more recently in the biomedical
sciences \citep{iqbal:2016}. The reasons for such non-reproducibility are as
diverse as these domains are. In medicine, the {\em study power and bias, the
number of other studies on the same question, and, importantly, the ratio of
true to no relationships among the relationships probed in each scientific
field} are important causes, as reported by \citep{ioannidis:2005}. In psychology,
the infamous p-value seems to be the root of all evil, while in the biomedical
sciences, \citep{iqbal:2016} identified a {\em lack of access to full datasets and
detailed protocols for both clinical and non-clinical biomedical
investigation}. Surprisingly enough, the computational sciences (in the broad
sense) and computer sciences are no exception, even though they rely mostly on
code and data that are believed to be not prone to the aforementioned
problems.\\

When \citep{collberg:2014, collberg:2015} decided to measure the
extent of the problem precisely, they investigated the availability of code and data,
and {\em the extent to which this code would actually build with reasonable
  effort}. The results were dramatic: of the 515 potentially reproducible
papers (out of 613) targeted by the study, the authors managed to ultimately run
only 102, i.e. less than 20\%. It is to be underlined that the authors only
investigated the possibility to run the code. They did not check for the
correctness of the code (does the code actually implement what is advertised
in the paper), nor the replicability of the results (does the run lead to the
same results as in the paper).  \citep{topalidou:2015a} encountered the same
problem when they tried to replicate a model from the computational
neuroscience literature: source code was not part of the supplementary section
of the paper and no link or repository was provided in the main text. When they
finally got their hands on the code, after contacting the corresponding
author, it was only to realize that it could not be compiled and was mostly
unusable.\\

Confronted with this problem, a small but growing number of journals and
publishers have decided to react and have adopted explicit data and software
policies. See for example the PLOS instructions on
\href{http://journals.plos.org/plosone/s/materials-and-software-sharing}{Materials
  and Software Sharing} and
on \href{http://journals.plos.org/plosone/s/data-availability}{Data Availability},
or the
\href{https://elifesciences.org/elife-news/inside-elife-forking-software-used-elife-papers-github}{recent
  annoucement} by eLife on forking software used in eLife papers to GitHub.
Such policies will ensure access to code and data in a well-defined format
\citep{perkel:2016} but this will not guarantee reproducilbility, nor
correctness. At the educational level, things have started to change with a
growing literature on best practices for making code reproducible
\citep{sandve:2013, crook:2013, wilson:2014, halchenko:2015, janz:2015,
  hinsen:2015}. Original initiatives such as Software and Data Carpentry
\citep{wilson:2016} are worth mentioning since their goal is {\em to make
  scientists more productive, and their work more reliable, by teaching them
  basic computing skills}. Obviously, such best practices could be applied as well to already published research software or code, provided the original
authors are willing to take on the challenge of re-implementing their own
software for the sake of better science. This is unlikely since, unfortunately, the
incentives for doing such time-consuming work are low or nonexistent.
Furthermore, if the original authors made mistakes in their original
implementation, chances are that they will reproduce the same mistakes again (just an educated guess, we have no data supporting this
assertion).\\

%% At this point, the question is what do we do? Do we have to declare
%% the research to be lost once and for all because the associated software or
%% code is nowhere to be found / lost / damaged / malfunctioning / not runnable /
%% not usable / does not give original results / etc. We would doom ourselves by
%% doing so

% As you may have guessed by now, this was the main motivation for
 %the creation of the ReScience journal\citep{^2].

\section*{Replication and reproduction}
\label{sec:replication-reproduction}
With the increasing recognition of the replication crisis as a problem
for scientific research, it has become a major subject of debate, but
unfortunately no common terminology has emerged so far. One reason for
the diverse use of terms is that each field of research has its own
specific technical and social obstacles on the road to published
results and findings that can be verified by other scientists. Here we
briefly summarize the obstacles that arise from the use of computers
and software in scientific research, and introduce the terminology we
will use in the rest of this article.\\

\textit{Reproducing} the result of a computation means running the
same software on the same input data and obtaining the same results.
The goal of a reproduction attempt is to verify that the computational
protocol leading to the results has been recorded correctly.
Performing computations reproducibly can be seen as a form of
provenance tracking, the software being a detailed record of all data
processing steps.

In theory, computation is a deterministic process and exact
reproduction should therefore be trivial. In reality, it is very
difficult to achieve because of the complexity of today's software
stacks and the tediousness of recording all interactions between a
scientist and a computer.  \citeauthor{Mesnard:2016} explain in
\citep{Mesnard:2016} how difficult it can be to reproduce a two-year
old computation even though all possible precautions were taken at the
time to ensure reproducibility.  The most frequent obstacles are the
loss of parts of the software or input data, lack of a computing
environment that is sufficiently similar to the one used initially,
and insufficient instructions for making the software work. An
obstacle specific to numerical computations is the use of
floating-point arithmetic, whose rules are subject to slightly
different interpretations by different compilers and runtime support
systems. A large variety of research practices and support tools have
been developed recently to facilitate reproducible computations. For a
collection of recipes that have proven useful, see
\citep{kitzes:2017}.

Publishing a reproducible computational result implies publishing all
the software and all the input data, or references to previously
published software and data, along with the traditional article
describing the work. An obvious added value is the availability of the
software and data, which helps readers to gain a better understanding
of the work, and can be re-used in other research projects. In
addition, reproducibly published results are more trustworthy, because
many common mistakes in working with computers can be excluded:
mistyping parameter values or input file names, updating the software
but forgetting to mention the changes in the description of the
method, planning to use one version of some software but actually
using a different one, etc.

Strictly speaking, reproducibility is defined in the context of
identical computational environments. However, useful scientific
software is expected to be robust with respect to certain changes in
this environment. A computer program that produces different results
when compiled using different compilers, or run on two different
computers, would be considered suspect by most practitioners, even if
it were demonstrably correct in one specific environment. Ultimately
it is not the software that is of interest for science, but the models
and methods that it implements. The software is merely a vehicle to
perform computations based on these models and methods. If results
depend on hard to control implementation details of the software,
their relation to the underlying models and methods becomes unclear
and unreliable.\\

\textit{Replicating} a published result means writing and then running
new software based on the description of a computational model or
method provided in the original publication, and obtaining results
that are similar enough to be considered equivalent. What exactly
``similar enough'' means strongly depends on the kind of computation
being performed, and can only be judged by an expert in the field.
The main obstacle to replicability is an incomplete or imprecise
description of the models and methods.

Replicability is a much stronger quality indicator than
reproducibility. In fact, reproducibility merely guarantees that all
the ingredients of a computation are well documented. It does not
imply that any of them are correct and/or appropriate for implementing
the models and methods that were meant to be applied, nor that the
descriptions of these models and methods are correct and clear. A
successful replication shows that two teams have produced independent
implementations that generate equivalent results, which makes serious
mistakes in either software unlikely. Moreover, it shows that the
second team was able to understand the description provided by the
first team.

Replication can be attempted both for reproducible and for
non-reproducible results. However, when an attempt to replicate
non-reproducible work fails, yielding results too different to be
considered equivalent, it can be very difficult to identify the cause
of the disagreement. Reproducibility guarantees the existence of a
precise and complete description of the models and methods being
applied in the original work, in the form of software source code,
which can be analyzed during the investigation of any
discrepancies. The holy grail of computational science is therefore a
reproducible replication of reproducible original work.

\section*{The ReScience initiative}

Performing a replication is a daunting task that is traditionally hardly or not
at all rewarded. Nevertheless, some people are actually willing to replicate
computational research. The motivations for doing so are very diverse (see Box 1). Students may want familiarize themselves with
a specific scientific domain, and acquire relevant practical experience by replicating important published work. Senior researchers may critically
need a specific piece of code for a research project and therefore re-implement a published computational method. If these people write a brand new
open source implementation of already published research, chances are that this new
implementation may be of great interest for other people as well, including
the original authors. The question is where to publish such a replication. To the
best of our knowledge, no major journal accepts publishing replications in computational science. This has been the main motivation for the
creation of the ReScience journal (\url{rescience.github.io}) by Konrad Hinsen
and Nicolas Rougier in 2015.\\

\begin{tcolorbox}[breakable, pad at break*=1mm,
                  colback=black!2.5, arc=0pt, outer arc=0pt, boxrule=.25pt]
\begin{footnotesize}
\textbf{Box 1.} Authors having  published in Rescience explain their motivation.\\

\textbf{(\cite{stachelek:2016})} I was motivated to replicate the results of
the original paper because I feel that working through code supplements to blog
posts has really helped me learn how to science. I could have published my
replication as a blog post but I wanted the exposure and permanency that goes
along with journal articles. This was my first experience with formal
replication. I think the review was useful because it forced me to consider how
the replication would be used by people other than myself. I have not yet
experienced any new interactions following publication. However, I did notify
the author of the original implementation about the replication's
publication. I think this may lead to future correspondence. The original
author suggested that he would consider submitting his own replications to
ReScience in the future.\\

\textbf{(\cite{topalidou:2015b})} Our initial motivation and the main reason
for replicating the model is that we needed it in order to collaborate with our
neurobiologist colleagues. When we arrived in our new lab, the model had just
been published (2013) but the original author had left the lab a few months before
our arrival. There was no public repository nor version control, and the paper
describing the model was incomplete and partly inaccurate. We managed to get
our hands on the original sources (6,000 lines of Delphi) only to realize we
could not compile them. It took us three months to replicate it using 250 lines
of Python. But at this time, there was no place to publish this kind of
replication to share the new code with colleagues. Since then, we have refined
the model and made new predictions that have been confirmed. Our initial
replication effort really gave the model a second life.\\

\textbf{(\cite{viejo:2016})} Replicating previous work is a relatively routine
task every time we want to build a new model: either because we want to build
on it, or because we want to compare to it. We also give replication tasks to
M.Sc. students every year, as projects. In all these cases, we are confronted
with incomplete or inaccurate model descriptions, as well as with the impossibility
to obtain the original results. Contacting the original authors sometimes
solves the problem, but not so often (because of the {\em dog ate my hard
  drive} syndrom). We thus accumulate knowledge, internal to the lab, about
which model works and which doesn't, and how a given model has to be parameterized
to really work. Without any place to publish it, this knowledge is
wasted. Publishing it in ReScience, opening the discussion publicly, will be a
progress for all of us. \par
\end{footnotesize}
\end{tcolorbox}

ReScience is an openly peer-reviewed journal that targets computational research
and encourages the explicit replication of already published research. In order
to provide the largest benefit to the scientific community, replications are
required to be reproducible and open-source. In less than two years of existence, 12
articles have been published and 3 are currently under review
(\href{https://github.com/ReScience/ReScience-submission/pull/20}{\#20},
\href{https://github.com/ReScience/ReScience-submission/pull/27}{\#27},
\href{https://github.com/ReScience/ReScience-submission/pull/30}{\#30}). The
editorial board covers a wide range of computational sciences (see
\url{http://rescience.github.io/board/}) and more than 70 volunteers have registered to be reviewers. The computational domains of published work
are in computational neuroscience, neuroimaging, computational ecology and
computer graphics with a majority in computational neuroscience. There is a
strong bias towards successful replication (100\%) and experience has
taught us that researchers are reluctant to publish failed replications, even when they can prove that the original work is wrong. For young researchers,
there is a social/professional risk in publishing articles that show
results from a senior researcher to be wrong. Until we implement a
certified anonymized submission process, this strong bias will most likely
remain.\\

One of the specificities of the ReScience journal is its publishing
chain that is radically different from any other traditional
scientific journals since ReScience lives on GitHub. One of the
consequences is that the whole process, from submission via reviewing
to publication, is open for anyone to see and even comment on.\\

Each submission takes the form of a pull request that is first
considered by a member of the editorial board, who may decide to
reject the submission if it does not respect the formal publication
criteria of ReScience. A submission must contain
\begin{itemize}
\item a precise reference to the work being replicated,
\item an explanation of why the authors think they have replicated the paper
      (same figures, same graphics,same behavior, etc.),
\item a description of any difficulties encountered during the
      replication,
\item open-source code that produces the replication results,
\item an explanation of this code for human readers.
\end{itemize}
A complete submission therefore consists of both code and an accompanying
article. Partial replications that cover only some of the results in the
original work are acceptable, but need to be justified.\\

If the submission respects these criteria, the editor assigns it to
two reviewers for further evaluation and tests. The reviewers evaluate
the code and the accompanying material in continuous interaction with
the authors through the discussion section until both reviewers
consider the work acceptable for publication. The goal of the review
is thus to help the authors meet the ReScience quality standards
through discussion. Since ReScience targets replication of already
published work, the criteria of relevance or novelty applied by most
traditional journals are irrelevant.

For a submission to be accepted, both reviewers must consider it
reproducible and a valid replication of the original work. As we
explained in section~\ref{sec:replication-reproduction}, this means
that the reviewers
\begin{itemize}
\item are able to run the proposed implementation on their computers,
\item obtain the same results as indicated in the accompanying paper,
\item consider these results sufficiently close to the ones reported in the original paper being replicated.
\end{itemize}
%
% The review concentrates on how easy it would be for another researcher
% to run the proposed implementation. This should be viewed in light of
% the standards in the field. If a given tool/library/software is
% mainstream in a field, it is ok to use them, but if a brand new
% standalone implementation is proposed, this must not rejected on this
% criterion.

Since independent implementation is a major criterion in replication
work, ReScience does not allow authors to submit replications of their
own research, nor the research of close collaborators. Mistakes in the
implementation of computational models and methods are often due to
biases that authors invariably have, consciously or not. Such biases
will inevitably carry over to a replication. Perhaps even more
importantly, cross-fertilization is generally useful in research, and
trying to replicate the work of oneâ€™s peers might pave the way for a
future collaboration, or may give rise to new ideas as a result of the
replication effort.



%% It has been said many times by many authors in the litterature that
%% reproducibility is the cornerstone of Science and we, as a scientific
%% community, should aim at such reproducibility. However, good intention are not
%% sufficient and a given computational results can be declared reproducible if
%% and only if it has been actually replicated in a the sense of a brand new
%% open-source and documented implementation.

%% ReScience already published 4 articles and as shown above, the original
%% motivations of these authors are all different and this might become even more
%% obvious and diverse with future publications. But, beyond these motivations,
%% publishing in ReScience may be especially important for students since this
%% represent a unique opportunity to show the community a given student is able to
%% read a scientific article, to have a deep understanding of it, to write a new
%% implementation and to eventually write a scientific article describing his/her
%% work. Although the ReScience publishing model is a bit different from other
%% academic journals, it can give students a first experience at peer-reviewed
%% scholarly publishing, including meeting standards of scientific rigor and
%% addressing reviewers' comments.  Furthermore, publishing in ReScience is a way
%% to actively contribute to open science while adding to one's publication
%% record.
%% \section*{Reproducibility is harder than you think}

%% \citeauthor{Mesnard:2016} explains in \citep{Mesnard:2016} that performing
%% reproducible and replicable simulations in computational fluid dynamics is harder
%% than you think. In fact, there are several different reasons for
%% non-reproducibility that, for the most part, are specific to each
%% scientifc domain. However, from our publishing experience, there are also some
%% common behaviors that can be identified. Missing code and/or data, unknown
%% dependencies, inaccurate or imprecise description appears to be shared
%% characteristic of a lot of non-reproducible work.\\

%% To be extended...

%\Discussion*{Discussion}
%\subsection*{An Utopia for Tomorrow}

\begin{figure}
  \includegraphics[width=1.0\columnwidth]{CoScience}
  \caption{\textbf{A} The ReScience publication chain starts from an
    original research by authors A, published in a journal, in
    conference proceedings, or as a preprint. This article constitutes
    the base material for authors B who attempt to replicate the work
    based on its description. Success or failure to replicate is not a
    criterion for acceptance or rejection, even though failure to
    replicate requires more precaution to ensure this is not a
    misunderstanding or a bug in the new code. After review, the
    replication is published, and feedback is given to original
    authors (and editors) to inform them the work has been replicated
    (or not). \textbf{B} The CoScience proposal would require the
    replication to happen \textit{before} the actual publication. In
    case of failure, nothing will be published. In case of success,
    the publication will be endorsed by authors A and authors B with
    identified roles and will be certified as reproducible because it
    has been replicated by an independent group.}
  \label{fig:coscience}
\end{figure}


\section*{Discussion}

% -> Short-term vs long-term reproducibility

There are several different reasons for non-reproducibility. For the most part,
those reasons are specific to each scientifc domain but, from our publishing
experience, there are also some common behaviors that can be identified.
Missing code and/or data, unknown dependencies, inaccurate or imprecise
description appears to be shared characteristic of a lot of non-reproducible
work. If the code accompanying a ReScience replication can fix a lot of these
issue, it can nonetheless only ensure short-term reproducibility. Even if a
piece of code is written following best practices, and then reviewed and
tested, it suffers from the same problem as any other piece of code, i.e. it
depends on a software stack whose stability is not guaranteed in the long
term. \citep{Mesnard:2016} illustrated this problem very clearly when they
tried to reproduce their own work performed two years earlier. Even though
Barba's group is committed to reproducible research practices, they did not
escape the many problems one can face when trying to re-run a piece of code.
This means the newly produced code for ReScience will likely be obsolete at
some point in the future. It can be a matter of months or years, or even
decades, but ultimately, obsolescence is inevitable. Therefore, the long-term
value of a ReScience publication is not the actual code but the accompanying
article. If you consider the original article with missing or incomplete
information and the new article with missing and checked information, you have
a complete and consistent description of the original work. 5, 10 or 20 years
from now, anyone should be able to replicate the work thanks to these two
articles. Of course, the new code can also help, but the true value of a
replication is really the accompanying article.\\

One immediate and legitimate question is to wonder to what extent such
replication could be performed prior to the publication of the original
article. This would strongly reinforce a claim because a successful and
independent replication would have been performed prior to the publication. As
illustrated on figure \ref{fig:coscience}, this would require for a group A to
contact a group B and to send them their a draft of their original work (the
one that would be normally submitted to a journal) such that group B could run
a replication and confirm or infirm the work. In case of confirmation, a
certified article could be later published with both group as authors (each
group being identified according to their respective roles). However, if the
replication failed and the original work cannot be fixed, this would prevent
the publication. This model would improve the overall reproducibiloty and would
slow down considerably the pace of publication we're observing
today. Unfortunately, such scenario is highly improbable. The pressure to
publish is so strong and the incentive for doing replication so low that it
would most probably prevent such collaborative work. At this point, CoScience
is mostly an utopia for tomorrow.

% -> Long term storage: Software Heritage

% -> Computational vs Experimental: ReScience X (Etienne Roesch)

% -> Post vs Pre replication: CoScience, an utopia for tomorrow


% -------------------------------------------------------------- References ---
\renewcommand*{\bibfont}{\footnotesize}
\printbibliography[title=References]


\end{document}

